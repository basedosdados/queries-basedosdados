{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pysus"
      ],
      "metadata": {
        "id": "y4pASUvt9ZwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN-NYU4Q9Ji_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Carregamento das bases\n",
        "from pysus.ftp.databases.sinan import SINAN\n",
        "sinan = SINAN().load()\n",
        "files = sinan.get_files(dis_code=\"CHIK\", year= range(2015, 2025))\n",
        "sinan.download(files, local_dir=\"/content/basedosdados/br_ms_sinan/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função de particionamento"
      ],
      "metadata": {
        "id": "dBbiwfSiYu6p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWjseqzt9JjB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "\n",
        "import logging\n",
        "import re\n",
        "from datetime import datetime\n",
        "from os import getenv, walk\n",
        "from os.path import join\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from uuid import uuid4\n",
        "def to_partitions(\n",
        "    data: pd.DataFrame,\n",
        "    partition_columns: List[str],\n",
        "    savepath: str,\n",
        "    file_type: str = \"csv\",\n",
        "):\n",
        "    \"\"\"Save data in to hive patitions schema, given a dataframe and a list of partition columns.\n",
        "    Args:\n",
        "        data (pandas.core.frame.DataFrame): Dataframe to be partitioned.\n",
        "        partition_columns (list): List of columns to be used as partitions.\n",
        "        savepath (str, pathlib.PosixPath): folder path to save the partitions.\n",
        "        file_type (str): default to csv. Accepts parquet.\n",
        "    Exemple:\n",
        "        data = {\n",
        "            \"ano\": [2020, 2021, 2020, 2021, 2020, 2021, 2021,2025],\n",
        "            \"mes\": [1, 2, 3, 4, 5, 6, 6,9],\n",
        "            \"sigla_uf\": [\"SP\", \"SP\", \"RJ\", \"RJ\", \"PR\", \"PR\", \"PR\",\"PR\"],\n",
        "            \"dado\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",'h'],\n",
        "        }\n",
        "        to_partitions(\n",
        "            data=pd.DataFrame(data),\n",
        "            partition_columns=['ano','mes','sigla_uf'],\n",
        "            savepath='partitions/',\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(data, (pd.core.frame.DataFrame)):\n",
        "        savepath = Path(savepath)\n",
        "        # create unique combinations between partition columns\n",
        "        unique_combinations = (\n",
        "            data[partition_columns]\n",
        "            # .astype(str)\n",
        "            .drop_duplicates(subset=partition_columns).to_dict(orient=\"records\")\n",
        "        )\n",
        "\n",
        "        for filter_combination in unique_combinations:\n",
        "            patitions_values = [\n",
        "                f\"{partition}={value}\"\n",
        "                for partition, value in filter_combination.items()\n",
        "            ]\n",
        "\n",
        "            # get filtered data\n",
        "            df_filter = data.loc[\n",
        "                data[filter_combination.keys()]\n",
        "                .isin(filter_combination.values())\n",
        "                .all(axis=1),\n",
        "                :,\n",
        "            ]\n",
        "            df_filter = df_filter.drop(columns=partition_columns)\n",
        "\n",
        "            # create folder tree\n",
        "            filter_save_path = Path(savepath / \"/\".join(patitions_values))\n",
        "            filter_save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            if file_type == \"csv\":\n",
        "                # append data to csv\n",
        "                file_filter_save_path = Path(filter_save_path) / \"data.csv\"\n",
        "                df_filter.to_csv(\n",
        "                    file_filter_save_path,\n",
        "                    sep=\",\",\n",
        "                    encoding=\"utf-8\",\n",
        "                    na_rep=\"\",\n",
        "                    index=False,\n",
        "                    mode=\"a\",\n",
        "                    header=not file_filter_save_path.exists(),\n",
        "                )\n",
        "            elif file_type == \"parquet\":\n",
        "                # append data to parquet\n",
        "                file_filter_save_path = Path(filter_save_path) / \"data.parquet\"\n",
        "                if file_filter_save_path.exists():\n",
        "                    existing_data = pd.read_parquet(file_filter_save_path)\n",
        "                    df_filter = pd.concat([existing_data, df_filter], ignore_index=True)\n",
        "                df_filter.to_parquet(\n",
        "                    file_filter_save_path, index=False, compression=\"gzip\"\n",
        "                )\n",
        "    else:\n",
        "        raise BaseException(\"Data need to be a pandas DataFrame\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Particionar todos os arquivos"
      ],
      "metadata": {
        "id": "oyvhAEnQY1Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for anos in range(15, 25):\n",
        "    ano_str = str(anos).zfill(2)\n",
        "    path = f'/content/basedosdados/br_ms_sinan/CHIKBR{ano_str}.parquet'\n",
        "\n",
        "    df0 = pd.read_parquet(path)\n",
        "    df0 = df0.astype(str)\n",
        "    df0['ano'] = '20' + str(path[40:43])\n",
        "    print(path[40:43])\n",
        "\n",
        "    lista = [\n",
        "    'ano',\n",
        "    'TP_NOT',\n",
        "    'ID_AGRAVO',\n",
        "    'DT_NOTIFIC',\n",
        "    'SEM_NOT',\n",
        "    'SG_UF_NOT',\n",
        "    'ID_REGIONA',\n",
        "    'ID_MUNICIP',\n",
        "    'ID_UNIDADE',\n",
        "    'DT_SIN_PRI',\n",
        "    'SEM_PRI',\n",
        "    'ID_PAIS',\n",
        "    'SG_UF',\n",
        "    'ID_RG_RESI',\n",
        "    'ID_MN_RESI',\n",
        "    'ANO_NASC',\n",
        "    'NU_IDADE_N',\n",
        "    'CS_SEXO',\n",
        "    'CS_RACA',\n",
        "    'CS_ESCOL_N',\n",
        "    'ID_OCUPA_N',\n",
        "    'CS_GESTANT',\n",
        "    'AUTO_IMUNE',\n",
        "    'DIABETES',\n",
        "    'HEMATOLOG',\n",
        "    'HEPATOPAT',\n",
        "    'RENAL',\n",
        "    'HIPERTENSA',\n",
        "    'ACIDO_PEPT',\n",
        "    'DT_INVEST',\n",
        "    'FEBRE',\n",
        "    'CEFALEIA',\n",
        "    'EXANTEMA',\n",
        "    'DOR_COSTAS',\n",
        "    'MIALGIA',\n",
        "    'VOMITO',\n",
        "    'NAUSEA',\n",
        "    'CONJUNTVIT',\n",
        "    'DOR_RETRO',\n",
        "    'ARTRALGIA',\n",
        "    'ARTRITE',\n",
        "    'LEUCOPENIA',\n",
        "    'EPISTAXE',\n",
        "    'PETEQUIA_N',\n",
        "    'GENGIVO',\n",
        "    'METRO',\n",
        "    'HEMATURA',\n",
        "    'SANGRAM',\n",
        "    'COMPLICA',\n",
        "    'LACO',\n",
        "    'HOSPITALIZ',\n",
        "    'DT_INTERNA',\n",
        "    'UF',\n",
        "    'MUNICIPIO',\n",
        "    'ALRM_HIPOT',\n",
        "    'ALRM_PLAQ',\n",
        "    'ALRM_VOM',\n",
        "    'ALRM_SANG',\n",
        "    'ALRM_HEMAT',\n",
        "    'ALRM_ABDOM',\n",
        "    'ALRM_LETAR',\n",
        "    'ALRM_HEPAT',\n",
        "    'ALRM_LIQ',\n",
        "    'DT_ALRM',\n",
        "    'GRAV_PULSO',\n",
        "    'GRAV_CONV',\n",
        "    'GRAV_ENCH',\n",
        "    'GRAV_INSUF',\n",
        "    'GRAV_TAQUI',\n",
        "    'GRAV_EXTRE',\n",
        "    'GRAV_HIPOT',\n",
        "    'GRAV_HEMAT',\n",
        "    'GRAV_MELEN',\n",
        "    'GRAV_METRO',\n",
        "    'GRAV_SANG',\n",
        "    'GRAV_AST',\n",
        "    'GRAV_MIOC',\n",
        "    'GRAV_CONSC',\n",
        "    'GRAV_ORGAO',\n",
        "    'PLAQ_MENOR',\n",
        "    'DT_CHIK_S1',\n",
        "    'RES_CHIKS1',\n",
        "    'RES_CHIKS2',\n",
        "    'RESUL_PRNT',\n",
        "    'DT_NS1',\n",
        "    'RESUL_NS1',\n",
        "    'DT_VIRAL',\n",
        "    'RESUL_VI_N',\n",
        "    'DT_PCR',\n",
        "    'RESUL_PCR_',\n",
        "    'RESUL_SORO',\n",
        "    'DT_SORO',\n",
        "    'SOROTIPO',\n",
        "    'HISTOPA_N',\n",
        "    'IMUNOH_N',\n",
        "    'MANI_HEMOR',\n",
        "    'CLASSI_FIN',\n",
        "    'CRITERIO',\n",
        "    'CON_FHD',\n",
        "    'TPAUTOCTO',\n",
        "    'COPAISINF',\n",
        "    'COUFINF',\n",
        "    'COMUNINF',\n",
        "    'DOENCA_TRA',\n",
        "    'CLINC_CHIK',\n",
        "    'EVOLUCAO',\n",
        "    'DT_OBITO',\n",
        "    'DT_ENCERRA',\n",
        "    'TP_SISTEMA',\n",
        "    'DT_DIGITA',\n",
        "    'DT_CHIK_S2',\n",
        "    'DT_PRNT',\n",
        "    'DT_GRAV'\n",
        "        ]\n",
        "    for x in lista:\n",
        "        if x not in df0.columns:\n",
        "            df0[x] = None\n",
        "\n",
        "    df0 = df0[lista]\n",
        "\n",
        "    df0.rename(columns={\n",
        "        'NU_ANO' : 'ano',\n",
        "        'SG_UF_NOT' : 'sigla_uf_notificacao'\n",
        "    }, inplace=True)\n",
        "\n",
        "    df0 = df0.loc[:,~df0.columns.duplicated()]\n",
        "    print(f'Particionando: {ano_str}...')\n",
        "    to_partitions(\n",
        "        data=df0,\n",
        "        partition_columns=['ano'],\n",
        "        savepath= \"/content/basedosdados/br_ms_sinan/partitions/\",\n",
        "        file_type= 'parquet',\n",
        "    )\n",
        "    del df0\n",
        "    print(f'Terminou de particionar: {ano_str}...')"
      ],
      "metadata": {
        "id": "yCL1zTP2hspV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rhJ-nNUVYdTy",
        "IG-bnJnOYkMR",
        "xUGzDaoPdecA",
        "Q75FoTxUL2R-",
        "0yH7YWCtNdM6",
        "dBbiwfSiYu6p",
        "oyvhAEnQY1Tl",
        "dnu3KIk8SGsE"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
